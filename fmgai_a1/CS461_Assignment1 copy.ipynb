{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"background-color:#FFFFFF\">   \n",
    "  <tr>     \n",
    "  <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/95/Logo_EPFL_2019.svg\" width=\"150x\"/>\n",
    "  </td>     \n",
    "  <td>\n",
    "  <h1> <b>CS-461: Foundation Models and Generative AI</b> </h1>\n",
    "  Prof. Charlotte Bunne  \n",
    "  </td>   \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Graded Assignment 1  \n",
    "### CS-461: Foundation Models and Generative AI - Fall 2025  - Due: October 8, 23:59 CET\n",
    "\n",
    "Welcome to the first graded assignment!\n",
    "In this assignment, you will **implement and explore self-supervised learning** on a downsampled subset of the [ImageNet-1k dataset](https://www.image-net.org/), and evaluate how well your model generalizes **both in-distribution and out-of-distribution (OOD)**.  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By completing this assignment, you will learn to:\n",
    "- Implement a custom **encoder** and **projection head** for images  \n",
    "- Experiment with **data augmentations** for self-supervised learning  \n",
    "- Train a model using a **self-supervised loss**  \n",
    "- Evaluate learned representations with **k-NN** and **linear probes**  \n",
    "- Assess **out-of-distribution (OOD) generalization** to unseen classes  \n",
    "- Save, visualize, and submit results in a reproducible way  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Practical Notes\n",
    "- **Dataset:**  \n",
    "  - Training: 200 ImageNet classes, 500 images each (100k total)  \n",
    "  - Validation: 200 ImageNet classes, 50 images each (10k total)  \n",
    "  - **OOD dataset:** 200 unseen classes, 50 images each (10k total)  \n",
    "- Use OOD only for **evaluation**, never for training.  \n",
    "- Checkpoints and evaluation intervals are already set up ‚Äî your main tasks are to fill in missing functions and customize the model.  \n",
    "- Some helper utilities (e.g., dataset loaders, probes) are provided in `utils.py`.  \n",
    "\n",
    "---\n",
    "\n",
    "üëâ **Deliverables:** You will submit:\n",
    "- Your modified **`models.py`**  \n",
    "- Trained weights in **`final_model.safetensors`**  \n",
    "- A short **report.md** (max 500 words) ‚Äî including **discussion of OOD results**  \n",
    "- This completed notebook **CS461_Assignment1.ipynb**  \n",
    "\n",
    "---\n",
    "\n",
    "‚ö†Ô∏è **Important:** Don‚Äôt forget to fill in your **SCIPER number** and **full name** in Section 0, otherwise you will receive **0 points**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import packages and set up the device. \\\n",
    "Feel free to add any additional packages you may need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reloads modules when you make changes (useful during development)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:64\"\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import copy, math, json, csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from safetensors.torch import save_model\n",
    "\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üÜî 0. SCIPER Number and Name  \n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT!** ‚ö†Ô∏è  \n",
    "You **must** fill in your **SCIPER number** and **full name** below.  \n",
    "\n",
    "This is **required for automatic grading**.  \n",
    "If you do **not** provide this information, you will receive **0Ô∏è‚É£ (zero)** for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIPER = \"395715\"\n",
    "LAST_NAME = \"Kroknes-Gomez\"\n",
    "FIRST_NAME = \"Yasmine\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datasets & Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the following, we will work with a subset of the ImageNet-1k dataset: color images downsampled to 64√ó64, covering 200 classes.\n",
    "- The training set contains 500 images per class (100,000 images in total), and the validation set contains 50 images per class (10,000 images in total).\n",
    "- The Out-Of-Distribution (OOD) datasets contain images from classes not present in the training set. It contains 50 images from 200 different classes (1,000 images in total).\n",
    "- The purpose of these OOD datasets is to evaluate the generalization capabilities of the learned representations. You should not use it for training.\n",
    "- During evalution, we will measure your model's performance on another OOD dataset (different from the one provided here), so make sure to not overfit on the provided OOD dataset.\n",
    "\n",
    "<!-- Let's download/load it and define a default transformation turning a PIL Image into a `torch.tensor` -->\n",
    "Make sure that you have access to the `/shared/CS461/cs461_assignment1_data/` folder. The folder structure should look like this:\n",
    "```\n",
    "cs461_assignment1_data/\n",
    "‚îî‚îÄ‚îÄ train.npz\n",
    "‚îî‚îÄ‚îÄ val.npz\n",
    "‚îî‚îÄ‚îÄ ood.npz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset class and other utilities you developed in previous homeworks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      "Extension for Scikit-learn* enabled (https://github.com/uxlfoundation/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from utils import ImageDatasetNPZ, default_transform, seed_all\n",
    "from utils import run_knn_probe, run_linear_probe, extract_features_and_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility, you can use the provided `seed_all` function to set the random seed for all relevant libraries (Python, NumPy, PyTorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(42)  # For reproducibility, you can use any integer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably want to implement custom data augmentations for the self-supervised learning method you choose. \\\n",
    "Feel free to swap the `default_transform` defined below and create multiple instances of datasets with different transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/shared/CS461/cs461_assignment1_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "class BYOLTransform:\n",
    "\n",
    "    def __init__(self, size=64, s=0.5, blur_p=0.5):\n",
    "        color_jitter = T.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
    "        k = 9\n",
    "        base = [\n",
    "            T.ToPILImage(), \n",
    "            T.RandomResizedCrop(size=size, scale=(0.3, 1.0)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomApply([color_jitter], p=0.8),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.RandomApply([T.GaussianBlur(kernel_size=k, sigma=(0.1, 2.0))], p=blur_p),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "        ]\n",
    "        self.train_transform = T.Compose(base)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.train_transform(x), self.train_transform(x)\n",
    "\n",
    "byol_transform = BYOLTransform()\n",
    "\n",
    "train_dataset = ImageDatasetNPZ(data_dir / 'train.npz', transform=byol_transform)\n",
    "val_dataset = ImageDatasetNPZ(data_dir / 'val.npz', transform=byol_transform)\n",
    "\n",
    "# train \"eval\" dataset that is single view to build the kNN/linear feature bank\n",
    "# train_eval_dataset = ImageDatasetNPZ(data_dir/'train.npz', transform=simclr_eval_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can split the provided OOD dataset into a training and validation set using the code below. \\\n",
    "You should not use the training split for actually training your models, but only for evaluation (e.g. kNN or linear probing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "ds_ood = ImageDatasetNPZ(data_dir / 'ood.npz', transform=default_transform)\n",
    "ood_val_ratio = 0.2\n",
    "train_mask = rng.permutation(len(ds_ood)) >= int(len(ds_ood) * ood_val_ratio)\n",
    "ds_oods_train = torch.utils.data.Subset(ds_ood, np.where(train_mask)[0])\n",
    "ds_oods_val = torch.utils.data.Subset(ds_ood, np.where(~train_mask)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_workers = 4\n",
    "pin_memory = True\n",
    "\n",
    "def byol_collate_fn(batch):\n",
    "    xs1, xs2, ys = [], [], []\n",
    "    for (x1, x2), y in batch:\n",
    "        xs1.append(x1)\n",
    "        xs2.append(x2)\n",
    "        ys.append(y)\n",
    "    return torch.stack(xs1), torch.stack(xs2), torch.tensor(ys)\n",
    "    \n",
    "collate_fn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=True, collate_fn=byol_collate_fn)\n",
    "val_loader  = DataLoader(val_dataset,  batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=False, collate_fn=byol_collate_fn)\n",
    "\n",
    "# train_eval_loader  = DataLoader(train_eval_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Your Model\n",
    "\n",
    "- Load your model from `models.py`.\n",
    "- You will need to modify the `encoder` and `projection` modules, as the provided template implementation is only a placeholder.\n",
    "- You SHOULD NOT change the `input_dim`, `input_channels`, and `feature_dim` parameters of the `ImageEncoder` class.\n",
    "- You can use an existing architecture (e.g., ResNet, ViT) but you SHOULD NOT use any pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Identity()\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (neck): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  (projector): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=2048, bias=False)\n",
       "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "    (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  )\n",
       "  (predictor): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=2048, bias=False)\n",
       "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import ImageEncoder\n",
    "\n",
    "model = ImageEncoder().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helpers for Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suggest you to implement the following helper functions to keep your training and evaluation loops clean and organized. \n",
    "- `training_step`: Performs a single training step (forward pass, loss computation, backward pass, optimizer step) and returns the loss value.\n",
    "- `evaluation_step`: Evaluates the model on the validation dataset and returns the accuracy.\n",
    "\n",
    "Depending on your specific requirements, you may also want to implement additional utility functions for tasks such as data loading, metric computation, and logging.\n",
    "\n",
    "As you have seen from previous assignments, loss functions for self-supervised learning objectives can be quite complex. \\\n",
    "Feel free to implement any helper functions you may need to compute the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(online_network, target_network, batch, optimizer, ema, device, scaler=None):\n",
    "    # TODO: Implement the training step\n",
    "    x1, x2, _ = batch\n",
    "    x1 = x1.to(device, non_blocking=True)\n",
    "    x2 = x2.to(device, non_blocking=True)\n",
    "\n",
    "    online_network.train()\n",
    "    target_network.eval()  # EMA target is inference-only\n",
    "\n",
    "    # AMP context (optional but convenient)\n",
    "    autocast_ctx = torch.amp.autocast('cuda') if scaler is not None else nullcontext()\n",
    "    with autocast_ctx:\n",
    "        # Online predictions on each view\n",
    "        _, _, p1 = online_network(x1)\n",
    "        _, _, p2 = online_network(x2)\n",
    "\n",
    "        # Target projections (stop-grad)\n",
    "        with torch.no_grad():\n",
    "            _, z1_t, _ = target_network(x1)  # predictor unused\n",
    "            _, z2_t, _ = target_network(x2)\n",
    "\n",
    "        # Symmetric BYOL loss\n",
    "        loss = byol_loss(p1, z2_t) + byol_loss(p2, z1_t)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    if scaler is not None:\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # EMA update of the target network (in-place)\n",
    "    with torch.no_grad():\n",
    "        ema.update(online_network, target_network)\n",
    "\n",
    "    return float(loss.item())\n",
    "\n",
    "\n",
    "def compute_byol_loss(online_network, target_network, batch, device, scaler=None):\n",
    "    x1, x2, _ = batch\n",
    "    x1 = x1.to(device, non_blocking=True)\n",
    "    x2 = x2.to(device, non_blocking=True)\n",
    "\n",
    "    online_network.train()\n",
    "    target_network.eval()\n",
    "\n",
    "    autocast_ctx = torch.amp.autocast('cuda') if (scaler is not None) else nullcontext()\n",
    "    with autocast_ctx:\n",
    "        # online predictions\n",
    "        _, _, p1 = online_network(x1)\n",
    "        _, _, p2 = online_network(x2)\n",
    "        # target projections (stop-grad)\n",
    "        with torch.no_grad():\n",
    "            _, z1_t, _ = target_network(x1)\n",
    "            _, z2_t, _ = target_network(x2)\n",
    "        # symmetric BYOL loss\n",
    "        loss = byol_loss(p1, z2_t) + byol_loss(p2, z1_t)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_step(online_network, \n",
    "                    target_network, \n",
    "                    train_loader_noaugment, \n",
    "                    test_loader_noaugment,\n",
    "                    device,\n",
    "                    use_target: bool = True,     \n",
    "                    normalize: bool = True,      \n",
    "                    do_linear: bool = True \n",
    "    ):\n",
    "    # TODO: Implement the evaluation step\n",
    "    eval_model = target_network if (use_target and target_network is not None) else online_network\n",
    "    eval_model.eval()\n",
    "\n",
    "    # 2) extract features (pre-projector; your get_features handles this)\n",
    "    train_feats_t, train_labels_t = extract_features_and_labels(eval_model, train_loader_noaugment, normalize=normalize)\n",
    "    test_feats_t,  test_labels_t  = extract_features_and_labels(eval_model, test_loader_noaugment,  normalize=normalize)\n",
    "\n",
    "    # sklearn expects numpy arrays\n",
    "    train_feats = train_feats_t.cpu().numpy()\n",
    "    test_feats  = test_feats_t.cpu().numpy()\n",
    "    train_labels = train_labels_t.cpu().numpy()\n",
    "    test_labels  = test_labels_t.cpu().numpy()\n",
    "\n",
    "    # 3) kNN probe (fast sanity metric)\n",
    "    knn_acc = run_knn_probe(train_feats, train_labels, test_feats, test_labels)\n",
    "\n",
    "    metrics = {\"knn_acc\": float(knn_acc)}\n",
    "\n",
    "    # 4) linear probe (stronger metric; optional to keep eval fast)\n",
    "    if do_linear:\n",
    "        linear_acc = run_linear_probe(train_feats, train_labels, test_feats, test_labels)\n",
    "        metrics[\"linear_acc\"] = float(linear_acc)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def byol_loss(p, z, eps: float = 1e-8):\n",
    "    \"\"\"2 - 2 * cosine similarity (batch mean).\"\"\"\n",
    "    p = F.normalize(p, dim=-1, eps=eps)\n",
    "    z = F.normalize(z, dim=-1, eps=eps)\n",
    "    return 2 - 2 * (p * z).sum(dim=-1).mean()\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, beta=0.99): self.beta = beta\n",
    "    @torch.no_grad()\n",
    "    def update(self, online, target):\n",
    "        for p_o, p_t in zip(online.parameters(), target.parameters()):\n",
    "            p_t.data.mul_(self.beta).add_(p_o.data, alpha=(1.0 - self.beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLogger:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def log(self, **kwargs):\n",
    "        self.history.append(dict(**kwargs))\n",
    "\n",
    "    def to_jsonl(self, path: Path):\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(path, \"w\") as f:\n",
    "            for row in self.history:\n",
    "                f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "    def to_csv(self, path: Path):\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if not self.history:\n",
    "            return\n",
    "        keys = sorted(self.history[0].keys())\n",
    "        with open(path, \"w\", newline=\"\") as f:\n",
    "            w = csv.DictWriter(f, fieldnames=keys)\n",
    "            w.writeheader()\n",
    "            for row in self.history:\n",
    "                w.writerow(row)\n",
    "\n",
    "# for visualisations\n",
    "def _denorm(x, mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
    "    m = torch.tensor(mean, device=x.device)[None, :, None, None]\n",
    "    s = torch.tensor(std,  device=x.device)[None, :, None, None]\n",
    "    return x * s + m\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_val_predictions(\n",
    "    eval_model,\n",
    "    train_loader_noaugment,\n",
    "    val_loader_noaugment,\n",
    "    out_path: Path,\n",
    "    n_samples: int = 16,\n",
    "    use_linear: bool = True\n",
    "):\n",
    "    eval_model.eval()\n",
    "\n",
    "    # 1) features\n",
    "    train_feats_t, train_labels_t = extract_features_and_labels(eval_model, train_loader_noaugment, normalize=True)\n",
    "    val_feats_t,   val_labels_t   = extract_features_and_labels(eval_model, val_loader_noaugment,   normalize=True)\n",
    "\n",
    "    train_feats = train_feats_t.numpy()\n",
    "    val_feats   = val_feats_t.numpy()\n",
    "    train_lbls  = train_labels_t.numpy()\n",
    "    val_lbls    = val_labels_t.numpy()\n",
    "\n",
    "    # 2) probe fit + accuracy\n",
    "    if use_linear:\n",
    "        clf = LogisticRegression(max_iter=1000, n_jobs=-1).fit(train_feats, train_lbls)\n",
    "    else:\n",
    "        clf = KNeighborsClassifier(n_neighbors=5, n_jobs=-1).fit(train_feats, train_lbls)\n",
    "\n",
    "    val_preds = clf.predict(val_feats)\n",
    "    acc = (val_preds == val_lbls).mean().item() if hasattr(acc, \"item\") else float((val_preds == val_lbls).mean())\n",
    "\n",
    "    # 3) gather n_samples images from val loader\n",
    "    imgs, labels = [], []\n",
    "    for x, y in val_loader_noaugment:\n",
    "        imgs.append(x)\n",
    "        labels.append(y)\n",
    "        if sum(b.size(0) for b in imgs) >= n_samples:\n",
    "            break\n",
    "    imgs = torch.cat(imgs, 0)[:n_samples]\n",
    "    labels = torch.cat(labels, 0)[:n_samples]\n",
    "\n",
    "    # 4) predict on sampled images\n",
    "    dev = next(eval_model.parameters()).device\n",
    "    feats_small = eval_model.get_features(imgs.to(dev))\n",
    "    feats_small = F.normalize(feats_small, dim=1).cpu().numpy()\n",
    "    preds_small = clf.predict(feats_small)\n",
    "\n",
    "    # optional probabilities if available\n",
    "    probs_small = None\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        probs_small = clf.predict_proba(feats_small).max(axis=1).tolist()\n",
    "\n",
    "    # 5) save grid\n",
    "    grid = make_grid(_denorm(imgs.to(dev)).cpu(), nrow=int(n_samples**0.5), padding=2)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    save_image(grid, out_path)\n",
    "\n",
    "    # 6) tabular sample info\n",
    "    samples = []\n",
    "    for i in range(n_samples):\n",
    "        row = {\"index\": int(i), \"true\": int(labels[i].item()), \"pred\": int(preds_small[i])}\n",
    "        if probs_small is not None:\n",
    "            row[\"conf\"] = float(probs_small[i])\n",
    "        samples.append(row)\n",
    "\n",
    "    return {\n",
    "        \"probe_acc\": float(acc),\n",
    "        \"samples\": samples,\n",
    "        \"grid_path\": str(out_path)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimizer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to adapt and add more arguments\n",
    "def param_groups_no_wd(model, weight_decay=1e-6):\n",
    "    decay, no_decay = [], []\n",
    "    for p in model.parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if p.ndim >= 2:\n",
    "            decay.append(p)\n",
    "        else:\n",
    "            no_decay.append(p)\n",
    "    return [\n",
    "        {\"params\": decay,    \"weight_decay\": weight_decay},\n",
    "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "def make_optimizer(model, global_batch):\n",
    "    lr = 1e-3 * (global_batch / 256.0)\n",
    "    return AdamW(param_groups_no_wd(model, weight_decay=1e-6), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "def make_scheduler(optimizer, total_epochs, steps_per_epoch, warmup_pct=0.1):\n",
    "    total_steps = total_epochs * steps_per_epoch\n",
    "    warmup_steps = max(1, int(warmup_pct * total_steps))\n",
    "    cosine_steps = max(1, total_steps - warmup_steps)\n",
    "\n",
    "    warmup = LinearLR(optimizer, start_factor=1e-3, end_factor=1.0, total_iters=warmup_steps)\n",
    "    cosine = CosineAnnealingLR(optimizer, T_max=cosine_steps, eta_min=0.0)\n",
    "    return SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_steps])\n",
    "\n",
    "def ema_momentum(epoch, total_epochs, m_start=0.99, m_end=0.996):\n",
    "    if total_epochs <= 1:\n",
    "        return m_end\n",
    "    t = epoch / (total_epochs - 1)\n",
    "    return m_end - (m_end - m_start) * 0.5 * (1.0 + math.cos(math.pi * t))\n",
    "\n",
    "\n",
    "# lr = 1e-3\n",
    "# weight_decay = 5e-2\n",
    "# lr_step_size = 10\n",
    "# lr_gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt your training configuration and implement the training loop. \\\n",
    "You probably want to save model checkpoints and evaluate the model on the validation set at regular intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50  # Adjust the number of epochs as needed\n",
    "eval_interval = 5  # Evaluate the model every 'eval_interval' epochs\n",
    "save_interval = 10  # Save the model every 'save_interval' epochs\n",
    "\n",
    "checkpoints_dir = Path('checkpoints')\n",
    "if not checkpoints_dir.exists():\n",
    "    checkpoints_dir.mkdir(parents=True, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 9.75 GiB of which 168.81 MiB is free. Process 1544998 has 2.15 GiB memory in use. Process 1845532 has 2.92 GiB memory in use. Process 2709594 has 7.44 GiB memory in use. Process 2747369 has 6.01 GiB memory in use. Process 2639663 has 1.13 GiB memory in use. Process 2787762 has 7.41 GiB memory in use. Of the allocated memory 7.29 GiB is allocated by PyTorch, and 16.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# forward-only (no step)\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_byol_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43monline_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# scale for accumulation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 53\u001b[0m, in \u001b[0;36mcompute_byol_loss\u001b[0;34m(online_network, target_network, batch, device, scaler)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_ctx:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# online predictions\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     _, _, p1 \u001b[38;5;241m=\u001b[39m online_network(x1)\n\u001b[0;32m---> 53\u001b[0m     _, _, p2 \u001b[38;5;241m=\u001b[39m \u001b[43monline_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# target projections (stop-grad)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/FMGAI_1/cs461_assignment1/models.py:80\u001b[0m, in \u001b[0;36mImageEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    Forward pass through the model.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m        torch.Tensor: Output embedding of shape (batch_size, proj_dim).\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m             \n\u001b[1;32m     81\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(h)             \n\u001b[1;32m     82\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(h)         \n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torchvision/models/resnet.py:158\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n\u001b[1;32m    161\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 9.75 GiB of which 168.81 MiB is free. Process 1544998 has 2.15 GiB memory in use. Process 1845532 has 2.92 GiB memory in use. Process 2709594 has 7.44 GiB memory in use. Process 2747369 has 6.01 GiB memory in use. Process 2639663 has 1.13 GiB memory in use. Process 2787762 has 7.41 GiB memory in use. Of the allocated memory 7.29 GiB is allocated by PyTorch, and 16.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "scaler = GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "online_network = model\n",
    "target_network = copy.deepcopy(online_network).to(device)\n",
    "for p in target_network.parameters(): \n",
    "    p.requires_grad = False\n",
    "\n",
    "global_batch = train_loader.batch_size  # adjust if using DDP: per_gpu * world_size\n",
    "base_lr = 1e-3 * (global_batch / 256.0)\n",
    "optimizer = AdamW(param_groups_no_wd(online_network, weight_decay=1e-6),\n",
    "                  lr=base_lr, betas=(0.9, 0.999))\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = max(1, n_epochs * steps_per_epoch)\n",
    "warmup_steps = max(1, int(0.1 * total_steps))\n",
    "cosine_steps = max(1, total_steps - warmup_steps)\n",
    "\n",
    "warmup = LinearLR(optimizer, start_factor=1e-3, end_factor=1.0, total_iters=warmup_steps)\n",
    "cosine = CosineAnnealingLR(optimizer, T_max=cosine_steps, eta_min=0.0)\n",
    "lr_scheduler = SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_steps])\n",
    "\n",
    "\n",
    "history = []\n",
    "def log_row(**kwargs): \n",
    "    history.append(dict(**kwargs))\n",
    "\n",
    "accum_steps = 4  # effective_batch = per_iter_batch * accum_steps\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    m = ema_momentum(epoch, n_epochs)\n",
    "    avg_loss = 0.0\n",
    "    online_network.train()\n",
    "    target_network.eval()\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        # forward-only (no step)\n",
    "        loss = compute_byol_loss(online_network, target_network, batch, device, scaler)\n",
    "        avg_loss += float(loss.item())\n",
    "\n",
    "        # scale for accumulation\n",
    "        loss = loss / accum_steps\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        do_step = (step % accum_steps == 0) or (step == len(train_loader))\n",
    "        if do_step:\n",
    "            if scaler is not None:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # EMA + LR scheduler once per optimizer step\n",
    "            with torch.no_grad():\n",
    "                EMA(beta=m).update(online_network, target_network)\n",
    "            lr_scheduler.step()\n",
    "\n",
    "\n",
    "    avg_loss /= max(1, len(train_loader))\n",
    "\n",
    "    row = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": float(avg_loss),\n",
    "        \"lr\": float(optimizer.param_groups[0][\"lr\"]),\n",
    "        \"ema_m\": float(m),\n",
    "    }\n",
    "\n",
    "    if (epoch + 1) % eval_interval == 0:\n",
    "        metrics = evaluation_step(\n",
    "            online_network, target_network,\n",
    "            train_loader_noaugment, val_loader_noaugment,\n",
    "            device, use_target=True, normalize=True, do_linear=True\n",
    "        )\n",
    "        row[\"knn_acc\"] = float(metrics[\"knn_acc\"])\n",
    "        row[\"linear_acc\"] = float(metrics.get(\"linear_acc\", float(\"nan\")))\n",
    "\n",
    "        grid_path = checkpoints_dir / f\"val_samples_epoch_{epoch+1:03d}.png\"\n",
    "        sample_info = sample_val_predictions(\n",
    "            target_network, train_loader_noaugment, val_loader_noaugment,\n",
    "            grid_path, n_samples=16, use_linear=True\n",
    "        )\n",
    "        row[\"sample_probe_acc\"] = float(sample_info[\"probe_acc\"])\n",
    "        row[\"sample_grid_path\"] = sample_info[\"grid_path\"]\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | loss {row['train_loss']:.4f} | \"\n",
    "              f\"kNN {row['knn_acc']:.2%} | linear {row['linear_acc']:.2%}\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    log_row(**row)\n",
    "\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        torch.save(online_network.state_dict(), checkpoints_dir / f\"online_epoch_{epoch+1:03d}.pth\")\n",
    "        torch.save(target_network.state_dict(), checkpoints_dir / f\"target_epoch_{epoch+1:03d}.pth\")\n",
    "\n",
    "# ---- final save + logs ----\n",
    "torch.save(online_network.state_dict(), checkpoints_dir / \"online_final.pth\")\n",
    "torch.save(target_network.state_dict(), checkpoints_dir / \"target_final.pth\")\n",
    "\n",
    "with open(checkpoints_dir / \"training_log.jsonl\", \"w\") as f:\n",
    "    for r in history:\n",
    "        f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "with open(checkpoints_dir / \"training_log.csv\", \"w\", newline=\"\") as f:\n",
    "    if history:\n",
    "        keys = sorted(history[0].keys())\n",
    "        w = csv.DictWriter(f, fieldnames=keys)\n",
    "        w.writeheader()\n",
    "        for r in history:\n",
    "            w.writerow(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the performance of your trained model, visualize some results. \\\n",
    "You can visualize:\n",
    "- Sample images from the validation set along with their predicted labels.\n",
    "- Training and validation loss curves over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize some results from your trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Submission Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must submit the following files:\n",
    "- `models.py`: Contains the implementation of your model architecture.\n",
    "- `final_model.safetensors`: The trained model weights saved in the safetensors format.\n",
    "- `report.md`: A brief report summarizing your approach, design choices, and results.\n",
    "- `CS461_Assignment1.ipynb`: The Jupyter notebook containing your code and explanations. Make sure to save your progress before running the cell below.\n",
    "\n",
    "You will submit your assignment under a single folder named `/home/cs461_assignment1_submission` containing the above files. \\\n",
    "Make sure to replace `<SCIPER>`, `<LAST_NAME>`, and `<FIRST_NAME>` with your actual SCIPER number, last name, and first name respectively. \\\n",
    "The following cell will help you move the files into the submission folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = Path('.')\n",
    "output_dir = Path.home() / 'cs461_assignment1_submission'\n",
    "\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir(parents=True, exist_ok=False)\n",
    "    \n",
    "shutil.copy(final_model_path, output_dir / 'final_model.safetensors')\n",
    "shutil.copy(work_dir / 'models.py', output_dir / 'models.py')\n",
    "shutil.copy(work_dir / 'CS461_Assignment1.ipynb', output_dir / 'CS461_Assignment1.ipynb')\n",
    "shutil.copy(work_dir / 'report.md', output_dir / 'report.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all required files are present in the submission folder before running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert SCIPER is not None and LAST_NAME is not None and FIRST_NAME is not None, \"Please set your SCIPER, LAST_NAME, and FIRST_NAME variables.\"\n",
    "\n",
    "list_of_files = ['final_model.safetensors', 'models.py', 'CS461_Assignment1.ipynb', 'report.md']\n",
    "files_found = all((output_dir / f).exists() for f in list_of_files)\n",
    "assert files_found, f\"One or more required files are missing in the submission folder: {list_of_files}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test whether your submission folder is appropriately structured by using the `eval.py`:\n",
    "```bash\n",
    "python eval.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment the line below to run the evaluation script and check your model's performance\n",
    "\n",
    "# !python eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "üéâ **Congratulations!**  \n",
    "You‚Äôve completed Assignment 1. Good luck, and don‚Äôt forget to double-check your submission!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
