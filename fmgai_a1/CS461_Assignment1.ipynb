{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"background-color:#FFFFFF\">   \n",
    "  <tr>     \n",
    "  <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/95/Logo_EPFL_2019.svg\" width=\"150x\"/>\n",
    "  </td>     \n",
    "  <td>\n",
    "  <h1> <b>CS-461: Foundation Models and Generative AI</b> </h1>\n",
    "  Prof. Charlotte Bunne  \n",
    "  </td>   \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Graded Assignment 1  \n",
    "### CS-461: Foundation Models and Generative AI - Fall 2025  - Due: October 8, 23:59 CET\n",
    "\n",
    "Welcome to the first graded assignment!\n",
    "In this assignment, you will **implement and explore self-supervised learning** on a downsampled subset of the [ImageNet-1k dataset](https://www.image-net.org/), and evaluate how well your model generalizes **both in-distribution and out-of-distribution (OOD)**.  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By completing this assignment, you will learn to:\n",
    "- Implement a custom **encoder** and **projection head** for images  \n",
    "- Experiment with **data augmentations** for self-supervised learning  \n",
    "- Train a model using a **self-supervised loss**  \n",
    "- Evaluate learned representations with **k-NN** and **linear probes**  \n",
    "- Assess **out-of-distribution (OOD) generalization** to unseen classes  \n",
    "- Save, visualize, and submit results in a reproducible way  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Practical Notes\n",
    "- **Dataset:**  \n",
    "  - Training: 200 ImageNet classes, 500 images each (100k total)  \n",
    "  - Validation: 200 ImageNet classes, 50 images each (10k total)  \n",
    "  - **OOD dataset:** 200 unseen classes, 50 images each (10k total)  \n",
    "- Use OOD only for **evaluation**, never for training.  \n",
    "- Checkpoints and evaluation intervals are already set up ‚Äî your main tasks are to fill in missing functions and customize the model.  \n",
    "- Some helper utilities (e.g., dataset loaders, probes) are provided in `utils.py`.  \n",
    "\n",
    "---\n",
    "\n",
    "üëâ **Deliverables:** You will submit:\n",
    "- Your modified **`models.py`**  \n",
    "- Trained weights in **`final_model.safetensors`**  \n",
    "- A short **report.md** (max 500 words) ‚Äî including **discussion of OOD results**  \n",
    "- This completed notebook **CS461_Assignment1.ipynb**  \n",
    "\n",
    "---\n",
    "\n",
    "‚ö†Ô∏è **Important:** Don‚Äôt forget to fill in your **SCIPER number** and **full name** in Section 0, otherwise you will receive **0 points**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import packages and set up the device. \\\n",
    "Feel free to add any additional packages you may need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Automatically reloads modules when you make changes (useful during development)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from safetensors.torch import save_model\n",
    "\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üÜî 0. SCIPER Number and Name  \n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT!** ‚ö†Ô∏è  \n",
    "You **must** fill in your **SCIPER number** and **full name** below.  \n",
    "\n",
    "This is **required for automatic grading**.  \n",
    "If you do **not** provide this information, you will receive **0Ô∏è‚É£ (zero)** for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIPER = \"395715\"\n",
    "LAST_NAME = \"Kroknes-Gomez\"\n",
    "FIRST_NAME = \"Yasmine\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datasets & Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the following, we will work with a subset of the ImageNet-1k dataset: color images downsampled to 64√ó64, covering 200 classes.\n",
    "- The training set contains 500 images per class (100,000 images in total), and the validation set contains 50 images per class (10,000 images in total).\n",
    "- The Out-Of-Distribution (OOD) datasets contain images from classes not present in the training set. It contains 50 images from 200 different classes (1,000 images in total).\n",
    "- The purpose of these OOD datasets is to evaluate the generalization capabilities of the learned representations. You should not use it for training.\n",
    "- During evalution, we will measure your model's performance on another OOD dataset (different from the one provided here), so make sure to not overfit on the provided OOD dataset.\n",
    "\n",
    "<!-- Let's download/load it and define a default transformation turning a PIL Image into a `torch.tensor` -->\n",
    "Make sure that you have access to the `/shared/CS461/cs461_assignment1_data/` folder. The folder structure should look like this:\n",
    "```\n",
    "cs461_assignment1_data/\n",
    "‚îî‚îÄ‚îÄ train.npz\n",
    "‚îî‚îÄ‚îÄ val.npz\n",
    "‚îî‚îÄ‚îÄ ood.npz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset class and other utilities you developed in previous homeworks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ImageDatasetNPZ, default_transform, seed_all\n",
    "from utils import run_knn_probe, run_linear_probe, extract_features_and_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility, you can use the provided `seed_all` function to set the random seed for all relevant libraries (Python, NumPy, PyTorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(42)  # For reproducibility, you can use any integer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably want to implement custom data augmentations for the self-supervised learning method you choose. \\\n",
    "Feel free to swap the `default_transform` defined below and create multiple instances of datasets with different transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/shared/CS461/cs461_assignment1_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoCropTransform:\n",
    "    def __init__(self, base_transform):\n",
    "        self.base = base_transform\n",
    "    def __call__(self, x):\n",
    "        return self.base(x), self.base(x)\n",
    "\n",
    "# two stochastic for train\n",
    "simclr_train_transform = T.Compose([\n",
    "    T.ToPILImage(), \n",
    "    T.RandomResizedCrop(64, scale=(0.08, 1.0)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomApply([T.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.GaussianBlur(kernel_size=9, sigma=(0.1, 2.0)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# lighter, deterministic for eval\n",
    "simclr_eval_transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize(64),\n",
    "    T.CenterCrop(64),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = ImageDatasetNPZ(data_dir / 'train.npz', transform=TwoCropTransform(simclr_train_transform))\n",
    "val_dataset = ImageDatasetNPZ(data_dir / 'val.npz', transform=simclr_eval_transform)\n",
    "\n",
    "# train \"eval\" dataset that is single view to build the kNN/linear feature bank\n",
    "train_eval_dataset = ImageDatasetNPZ(data_dir/'train.npz', transform=simclr_eval_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can split the provided OOD dataset into a training and validation set using the code below. \\\n",
    "You should not use the training split for actually training your models, but only for evaluation (e.g. kNN or linear probing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "ds_ood = ImageDatasetNPZ(data_dir / 'ood.npz', transform=default_transform)\n",
    "ood_val_ratio = 0.2\n",
    "train_mask = rng.permutation(len(ds_ood)) >= int(len(ds_ood) * ood_val_ratio)\n",
    "ds_oods_train = torch.utils.data.Subset(ds_ood, np.where(train_mask)[0])\n",
    "ds_oods_val = torch.utils.data.Subset(ds_ood, np.where(~train_mask)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_workers = 4\n",
    "pin_memory = True\n",
    "collate_fn = None  # Replace with your custom collate function if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader  = DataLoader(val_dataset,  batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "train_eval_loader  = DataLoader(train_eval_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Your Model\n",
    "\n",
    "- Load your model from `models.py`.\n",
    "- You will need to modify the `encoder` and `projection` modules, as the provided template implementation is only a placeholder.\n",
    "- You SHOULD NOT change the `input_dim`, `input_channels`, and `feature_dim` parameters of the `ImageEncoder` class.\n",
    "- You can use an existing architecture (e.g., ResNet, ViT) but you SHOULD NOT use any pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (neck): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  (projector): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=2048, bias=False)\n",
       "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "    (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import ImageEncoder\n",
    "\n",
    "model = ImageEncoder().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helpers for Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suggest you to implement the following helper functions to keep your training and evaluation loops clean and organized. \n",
    "- `training_step`: Performs a single training step (forward pass, loss computation, backward pass, optimizer step) and returns the loss value.\n",
    "- `evaluation_step`: Evaluates the model on the validation dataset and returns the accuracy.\n",
    "\n",
    "Depending on your specific requirements, you may also want to implement additional utility functions for tasks such as data loading, metric computation, and logging.\n",
    "\n",
    "As you have seen from previous assignments, loss functions for self-supervised learning objectives can be quite complex. \\\n",
    "Feel free to implement any helper functions you may need to compute the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, batch, optimizer, scaler=None, clip_grad_norm=None, amp_dtype=torch.float16):\n",
    "    # TODO: Implement the training step\n",
    "    model.train()\n",
    "    \n",
    "    (x1, x2), _ = batch\n",
    "    device = next(model.parameters()).device\n",
    "    x1 = x1.to(device, non_blocking=True)\n",
    "    x2 = x2.to(device, non_blocking=True)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    if scaler is not None:\n",
    "        with autocast('cuda', dtype=amp_dtype):\n",
    "            _, z1 = model(x1)\n",
    "            _, z2 = model(x2)\n",
    "            loss = custom_loss_function(z1, z2)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if clip_grad_norm is not None:\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "            \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    else:\n",
    "        _, z1 = model(x1)\n",
    "        _, z2 = model(x2)\n",
    "        loss = custom_loss_function(z1, z2)\n",
    "        loss.backward()\n",
    "\n",
    "        if clip_grad_norm is not None:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    return float(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_step(model, eval_loader, batch, do_linear=True):\n",
    "    # TODO: Implement the evaluation step\n",
    "    model.eval()\n",
    "\n",
    "    x_tr, y_tr = extract_features_and_labels(model, train_eval_loader, normalize=True)\n",
    "    x_va, y_va = extract_features_and_labels(model, val_loader, normalize=True)\n",
    "\n",
    "    x_tr, y_tr = x_tr.cpu().numpy(), y_tr.cpu().numpy()\n",
    "    x_va, y_va = x_va.cpu().numpy(), y_va.cpu().numpy()\n",
    "\n",
    "    knn_acc = run_knn_probe(x_tr, y_tr, x_va, y_va)\n",
    "    out = {\"knn_accuracy\": 100.0 * knn_acc}\n",
    "\n",
    "    if do_linear:\n",
    "        lin_acc = run_linear_probe(x_tr, y_tr, x_va, y_va)\n",
    "        out[\"linear_accuracy\"] = 100.0 * lin_acc\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_function(z1, z2, temperature: float = 0.1):\n",
    "    # TODO: Depend on your training paradigm, implement your custom loss function\n",
    "    B, D = z1.size()\n",
    "    z = torch.cat([z1, z2], dim=0)\n",
    "    sim = (z @ z.T)\n",
    "\n",
    "    logits = sim / temperature\n",
    "    diag = torch.eye(2*B, device=z.device, dtype=torch.bool)\n",
    "    logits = logits.masked_fill(diag, float('-inf'))\n",
    "\n",
    "    targets = torch.arange(B, device=z.device)\n",
    "    targets = torch.cat([targets + B, targets], dim=0)\n",
    "\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimizer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to adapt and add more arguments\n",
    "lr = 1e-3\n",
    "weight_decay = 5e-2\n",
    "lr_step_size = 10\n",
    "lr_gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt your training configuration and implement the training loop. \\\n",
    "You probably want to save model checkpoints and evaluate the model on the validation set at regular intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200  # Adjust the number of epochs as needed\n",
    "eval_interval = 5  # Evaluate the model every 'eval_interval' epochs\n",
    "save_interval = 10  # Save the model every 'save_interval' epochs\n",
    "\n",
    "checkpoints_dir = Path('checkpoints')\n",
    "if not checkpoints_dir.exists():\n",
    "    checkpoints_dir.mkdir(parents=True, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 4/200 [07:17<5:56:58, 109.28s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      "  2%|‚ñé         | 5/200 [10:26<7:28:23, 137.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 | Train Loss: 4.4382 | kNN-5: 1.52% | Linear: 2.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 9/200 [17:39<6:04:08, 114.39s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 | Train Loss: 3.5836 | kNN-5: 2.44% | Linear: 4.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 10/200 [21:04<7:31:00, 142.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_10.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 14/200 [28:27<6:05:54, 118.03s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      "  8%|‚ñä         | 15/200 [31:48<7:21:15, 143.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 | Train Loss: 3.2819 | kNN-5: 2.62% | Linear: 4.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñâ         | 19/200 [39:02<5:52:08, 116.73s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200 | Train Loss: 3.2368 | kNN-5: 2.98% | Linear: 4.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 20/200 [42:05<6:49:37, 136.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_20.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 24/200 [49:20<5:38:20, 115.35s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 12%|‚ñà‚ñé        | 25/200 [52:29<6:41:10, 137.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200 | Train Loss: 3.1847 | kNN-5: 2.97% | Linear: 4.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 29/200 [59:38<5:25:20, 114.15s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200 | Train Loss: 3.1798 | kNN-5: 2.84% | Linear: 4.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|‚ñà‚ñå        | 30/200 [1:02:39<6:20:14, 134.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_30.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|‚ñà‚ñã        | 34/200 [1:09:40<5:10:04, 112.08s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 18%|‚ñà‚ñä        | 35/200 [1:12:47<6:10:09, 134.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/200 | Train Loss: 3.1872 | kNN-5: 2.84% | Linear: 4.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñâ        | 39/200 [1:19:47<5:00:53, 112.13s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/200 | Train Loss: 3.1790 | kNN-5: 3.00% | Linear: 4.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 40/200 [1:23:03<6:06:06, 137.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_40.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 44/200 [1:30:09<4:56:33, 114.06s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 22%|‚ñà‚ñà‚ñé       | 45/200 [1:33:13<5:48:51, 135.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200 | Train Loss: 3.1834 | kNN-5: 2.66% | Linear: 4.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñç       | 49/200 [1:40:22<4:46:02, 113.66s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200 | Train Loss: 3.1660 | kNN-5: 2.99% | Linear: 4.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñå       | 50/200 [1:43:44<5:50:38, 140.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_50.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|‚ñà‚ñà‚ñã       | 54/200 [1:50:52<4:39:39, 114.93s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 28%|‚ñà‚ñà‚ñä       | 55/200 [1:54:04<5:33:24, 137.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200 | Train Loss: 3.1825 | kNN-5: 2.91% | Linear: 4.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñâ       | 59/200 [2:01:12<4:29:08, 114.53s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200 | Train Loss: 3.1805 | kNN-5: 2.84% | Linear: 5.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 60/200 [2:04:31<5:26:14, 139.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_60.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 64/200 [2:11:38<4:19:25, 114.45s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 32%|‚ñà‚ñà‚ñà‚ñé      | 65/200 [2:14:57<5:14:39, 139.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/200 | Train Loss: 3.1721 | kNN-5: 2.80% | Linear: 4.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|‚ñà‚ñà‚ñà‚ñç      | 69/200 [2:22:01<4:09:18, 114.19s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/200 | Train Loss: 3.1685 | kNN-5: 2.83% | Linear: 5.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|‚ñà‚ñà‚ñà‚ñå      | 70/200 [2:25:13<4:57:37, 137.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_70.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|‚ñà‚ñà‚ñà‚ñã      | 74/200 [2:32:16<3:57:56, 113.30s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 75/200 [2:35:25<4:43:50, 136.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/200 | Train Loss: 3.1686 | kNN-5: 2.96% | Linear: 4.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñâ      | 79/200 [2:42:29<3:48:13, 113.17s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200 | Train Loss: 3.1854 | kNN-5: 2.98% | Linear: 4.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 80/200 [2:46:00<4:45:08, 142.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_80.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 84/200 [2:53:04<3:42:13, 114.94s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 85/200 [2:56:15<4:24:00, 137.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/200 | Train Loss: 3.1824 | kNN-5: 2.78% | Linear: 4.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 89/200 [3:03:19<3:30:32, 113.80s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/200 | Train Loss: 3.1748 | kNN-5: 3.04% | Linear: 5.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 90/200 [3:06:38<4:15:15, 139.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_90.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 94/200 [3:13:40<3:21:12, 113.89s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 95/200 [3:16:46<3:57:13, 135.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/200 | Train Loss: 3.1719 | kNN-5: 2.88% | Linear: 3.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 99/200 [3:23:51<3:10:30, 113.17s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200 | Train Loss: 3.1784 | kNN-5: 2.69% | Linear: 5.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 100/200 [3:27:02<3:47:19, 136.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_100.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 104/200 [3:34:08<3:02:08, 113.84s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 105/200 [3:37:20<3:37:05, 137.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/200 | Train Loss: 3.1753 | kNN-5: 2.90% | Linear: 4.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 109/200 [3:44:26<2:52:42, 113.87s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/200 | Train Loss: 3.1824 | kNN-5: 2.75% | Linear: 4.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 110/200 [3:47:35<3:24:24, 136.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_110.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 114/200 [3:54:41<2:42:43, 113.53s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 115/200 [3:57:58<3:16:10, 138.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200 | Train Loss: 3.1696 | kNN-5: 2.79% | Linear: 5.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 119/200 [4:05:03<2:33:44, 113.89s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200 | Train Loss: 3.1791 | kNN-5: 2.95% | Linear: 4.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 120/200 [4:08:20<3:04:55, 138.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_120.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 124/200 [4:15:28<2:24:58, 114.46s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 125/200 [4:18:45<2:53:59, 139.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/200 | Train Loss: 3.1765 | kNN-5: 2.91% | Linear: 4.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 129/200 [4:25:46<2:14:10, 113.39s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/200 | Train Loss: 3.1798 | kNN-5: 2.78% | Linear: 4.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 130/200 [4:28:53<2:37:58, 135.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_130.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 134/200 [4:35:56<2:04:20, 113.03s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 135/200 [4:39:12<2:29:14, 137.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/200 | Train Loss: 3.1810 | kNN-5: 2.86% | Linear: 5.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 139/200 [4:46:18<1:55:39, 113.76s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/200 | Train Loss: 3.1729 | kNN-5: 2.82% | Linear: 5.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 140/200 [4:49:55<2:24:45, 144.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_140.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 144/200 [4:57:04<1:48:56, 116.73s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 145/200 [5:00:26<2:10:23, 142.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/200 | Train Loss: 3.1771 | kNN-5: 2.88% | Linear: 5.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 149/200 [5:07:34<1:38:17, 115.63s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/200 | Train Loss: 3.1733 | kNN-5: 2.83% | Linear: 4.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 150/200 [5:10:45<1:55:04, 138.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_150.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 154/200 [5:17:55<1:28:13, 115.08s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 155/200 [5:21:06<1:43:33, 138.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200 | Train Loss: 3.1877 | kNN-5: 2.87% | Linear: 4.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 159/200 [5:28:17<1:18:41, 115.16s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/200 | Train Loss: 3.1761 | kNN-5: 2.73% | Linear: 4.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 160/200 [5:31:23<1:30:53, 136.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_160.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 164/200 [5:38:30<1:08:07, 113.53s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 165/200 [5:41:40<1:19:32, 136.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/200 | Train Loss: 3.1698 | kNN-5: 2.87% | Linear: 4.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 169/200 [5:48:46<58:39, 113.53s/it]  hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/200 | Train Loss: 3.1842 | kNN-5: 2.85% | Linear: 4.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 170/200 [5:51:51<1:07:33, 135.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_170.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 174/200 [5:58:59<49:09, 113.46s/it]  hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 175/200 [6:02:04<56:15, 135.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200 | Train Loss: 3.1751 | kNN-5: 2.84% | Linear: 4.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 179/200 [6:09:10<39:41, 113.39s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/200 | Train Loss: 3.1754 | kNN-5: 2.84% | Linear: 5.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 180/200 [6:12:26<46:02, 138.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_180.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 184/200 [6:19:34<30:29, 114.36s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 185/200 [6:22:41<33:58, 135.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/200 | Train Loss: 3.1765 | kNN-5: 2.94% | Linear: 5.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 189/200 [6:29:44<20:40, 112.74s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/200 | Train Loss: 3.1668 | kNN-5: 2.96% | Linear: 4.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 190/200 [6:32:55<22:44, 136.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_190.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 194/200 [6:39:58<11:18, 113.02s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n",
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 195/200 [6:43:12<11:26, 137.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195/200 | Train Loss: 3.1829 | kNN-5: 2.83% | Linear: 4.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 199/200 [6:50:14<01:53, 113.22s/it]hwloc/linux: failed to find sysfs cpu topology directory, aborting linux discovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200 | Train Loss: 3.1793 | kNN-5: 2.92% | Linear: 5.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [6:53:29<00:00, 124.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at checkpoints/model_epoch_200.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "scaler = GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    # TODO: Implement the training and evaluation loop\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        loss_val = training_step(model, batch, optimizer, scaler)\n",
    "        running_loss += loss_val\n",
    "\n",
    "    avg_train_loss = running_loss / max(1, len(train_loader))\n",
    "\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    # if (epoch + 1) % eval_interval == 0:\n",
    "    #     model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         val_stats = evaluation_step(model, val_loader)  # should internally handle device\n",
    "    #     print(f\"Epoch {epoch+1}/{n_epochs} | \"\n",
    "    #           f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "    #           f\"Val kNN-5 Acc: {val_stats['knn_accuracy']:.2f}%\")\n",
    "\n",
    "    if (epoch + 1) % eval_interval == 0:\n",
    "        stats = evaluation_step(model, train_eval_loader, val_loader, do_linear=True)\n",
    "        line = (f\"Epoch {epoch+1}/{n_epochs} | \"\n",
    "                f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "                f\"kNN-5: {stats['knn_accuracy']:.2f}%\")\n",
    "        if 'linear_accuracy' in stats:\n",
    "            line += f\" | Linear: {stats['linear_accuracy']:.2f}%\"\n",
    "        print(line)\n",
    "\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        checkpoint_path = checkpoints_dir / f\"model_epoch_{epoch+1}.safetensors\"\n",
    "        save_model(model, checkpoint_path)\n",
    "        print(f\"Model checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "# Save the final model\n",
    "final_model_path = checkpoints_dir / 'model_final.safetensors'\n",
    "save_model(model, final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the performance of your trained model, visualize some results. \\\n",
    "You can visualize:\n",
    "- Sample images from the validation set along with their predicted labels.\n",
    "- Training and validation loss curves over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize some results from your trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Submission Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must submit the following files:\n",
    "- `models.py`: Contains the implementation of your model architecture.\n",
    "- `final_model.safetensors`: The trained model weights saved in the safetensors format.\n",
    "- `report.md`: A brief report summarizing your approach, design choices, and results.\n",
    "- `CS461_Assignment1.ipynb`: The Jupyter notebook containing your code and explanations. Make sure to save your progress before running the cell below.\n",
    "\n",
    "You will submit your assignment under a single folder named `/home/cs461_assignment1_submission` containing the above files. \\\n",
    "Make sure to replace `<SCIPER>`, `<LAST_NAME>`, and `<FIRST_NAME>` with your actual SCIPER number, last name, and first name respectively. \\\n",
    "The following cell will help you move the files into the submission folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = Path('.')\n",
    "output_dir = Path.home() / 'cs461_assignment1_submission'\n",
    "\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir(parents=True, exist_ok=False)\n",
    "    \n",
    "shutil.copy(final_model_path, output_dir / 'final_model.safetensors')\n",
    "shutil.copy(work_dir / 'models.py', output_dir / 'models.py')\n",
    "shutil.copy(work_dir / 'CS461_Assignment1.ipynb', output_dir / 'CS461_Assignment1.ipynb')\n",
    "shutil.copy(work_dir / 'report.md', output_dir / 'report.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all required files are present in the submission folder before running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert SCIPER is not None and LAST_NAME is not None and FIRST_NAME is not None, \"Please set your SCIPER, LAST_NAME, and FIRST_NAME variables.\"\n",
    "\n",
    "list_of_files = ['final_model.safetensors', 'models.py', 'CS461_Assignment1.ipynb', 'report.md']\n",
    "files_found = all((output_dir / f).exists() for f in list_of_files)\n",
    "assert files_found, f\"One or more required files are missing in the submission folder: {list_of_files}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test whether your submission folder is appropriately structured by using the `eval.py`:\n",
    "```bash\n",
    "python eval.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment the line below to run the evaluation script and check your model's performance\n",
    "\n",
    "# !python eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "üéâ **Congratulations!**  \n",
    "You‚Äôve completed Assignment 1. Good luck, and don‚Äôt forget to double-check your submission!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
